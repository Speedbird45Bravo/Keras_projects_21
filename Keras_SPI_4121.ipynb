{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_SPI_4121.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPICkTAUYXzP4gZ/z7thJsh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipsY4ETEii7U"
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# This is a swing at a custom neural network predicting soccer game results based on scores.\n",
        "# I've been in Keras for a few months, but am just starting to understand my way around.\n",
        "df = pd.read_csv(\"https://projects.fivethirtyeight.com/soccer-api/club/spi_matches.csv\").dropna()\n",
        "\n",
        "# We need to calculate one team's aggregate margin in order to determine the result.\n",
        "# We've chosen home for that calculation, but may as well get away as well for good measure.\n",
        "# Then, the absolute margin is the number of goals between the teams.\n",
        "df['h_marg'] = df['score1'] - df['score2']\n",
        "df['a_marg'] = df['score2'] - df['score1']\n",
        "df['margin'] = np.abs(df['score1'] - df['score2'])\n",
        "\n",
        "results = []\n",
        "\n",
        "for i in df['h_marg']:\n",
        "  if i > 0: # If the home team's margin is greater than 0, it's a home win.\n",
        "    results.append(\"HOME WIN\")\n",
        "  elif i < 0: # If the home team's margin is less than 0, it's an away win.\n",
        "    results.append(\"AWAY WIN\")\n",
        "  else: # Otherwise, it's a draw.\n",
        "    results.append(\"DRAW\")\n",
        "\n",
        "df['result'] = results"
      ],
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1SHdtXTr0Y6"
      },
      "source": [
        "# The model gets a little confused the more numerical rows are fed into it, so we're keeping it simple.\n",
        "X = df[['score1', 'score2', 'margin']].reset_index(drop=True)\n",
        "y = df[['result']].copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6)"
      ],
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDtLz66a8GTP"
      },
      "source": [
        "# Now it's time to make some random test data (separate from the existing test dfs) to use for predictions and accuracy. \n",
        "# First, we need to generate a function to turn the list into a dataframe.\n",
        "\n",
        "def to_df(l1):\n",
        "  df = pd.DataFrame(l1).reset_index(drop=True)\n",
        "  return df\n",
        "\n",
        "def to_cat(cats):\n",
        "  cat = to_categorical(pd.factorize(cats)[0])\n",
        "  return cat\n",
        "\n",
        "# These scores are just # of goals (integers between 0 and 4), so we can use randint for this.\n",
        "t1 = to_df(np.random.randint(0,4,size=25))\n",
        "t2 = to_df(np.random.randint(0,4,size=25))\n",
        "t3 = np.abs(t1) - np.abs(t2)\n",
        "t4 = t2 - t1\n",
        "t5 = t1 - t2\n",
        "\n",
        "t_res = []\n",
        "\n",
        "cols = ['score1', 'score2', 'margin', 'h_margin', 'a_margin']\n",
        "\n",
        "rand_X_test = pd.concat([t1, t2, t3, t4, t5], axis=1)\n",
        "rand_X_test.columns = cols\n",
        "\n",
        "for i in rand_X_test['h_margin']:\n",
        "  if i > 0: # If the home team's margin is greater than 0, it's a home win.\n",
        "    t_res.append(\"HOME WIN\")\n",
        "  elif i < 0: # If the home team's margin is less than 0, it's an away win.\n",
        "    t_res.append(\"AWAY WIN\")\n",
        "  else: # Otherwise, it's a draw.\n",
        "    t_res.append(\"DRAW\")\n",
        "\n",
        "rand_X_test['result'] = t_res\n",
        "rand_y_test = to_cat(rand_X_test['result'])\n",
        "rand_X_test = rand_X_test[['score1', 'score2', 'margin']]\n",
        "rand_reshape = rand_X_test.shape[0] * rand_X_test.shape[1]\n",
        "rand_X_test = np.asarray(rand_X_test)\n",
        "rand_y_test = np.asarray(rand_y_test)"
      ],
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf8bCj8SD5R4"
      },
      "source": [
        "# Have to factorize the labels before converting them to categorical (e.g. 1.,0.,0.).\n",
        "y_train = to_cat(y_train['result'])\n",
        "y_test = to_cat(y_test['result'])\n",
        "\n",
        "# The train and test data will be easier to handle in array form.\n",
        "X_train = np.asarray(X_train)\n",
        "X_test = np.asarray(X_test)"
      ],
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq9DJLDYD90V"
      },
      "source": [
        "# Two layers with a 3 unit output layer given the number of outcomes (HOME WIN, AWAY WIN, DRAW).\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF8WnMkZD-Km",
        "outputId": "f4b35c64-4581-4c3b-99e4-5e695d68028c"
      },
      "source": [
        "# Definitely excessive on the epochs. ¯\\_(ツ)_/¯\n",
        "model.fit(X_train, y_train, epochs=120, batch_size=128, validation_split=0.2, verbose=0)"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9f8b652ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8_qMHJaEHDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99082c9-0cf1-4c58-d3fd-0789f900a4fc"
      },
      "source": [
        "# Now we will predict results based on the random data we generated.\n",
        "predictions = model.predict(rand_X_test).round(2)\n",
        "predictions = np.asarray(predictions)"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9fa568bc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbykGDS5EJYG"
      },
      "source": [
        "# Reshaping y_test and predictions to be directly compared 1v1 iteratively.\n",
        "predictions = predictions.reshape(rand_reshape,)\n",
        "predictions = pd.DataFrame(predictions).reset_index(drop=True)\n",
        "rand_y_test = rand_y_test.reshape(rand_reshape,)\n",
        "rand_y_test = pd.DataFrame(rand_y_test).reset_index(drop=True)\n",
        "PvA = pd.concat([rand_y_test, predictions], axis=1)\n",
        "PvA.columns = [\"Predicted\", \"Actual\"]"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_LV-1FeELB2"
      },
      "source": [
        "# If the margin is 0, there is no error. If the margin is 1, there was an error.\n",
        "PvA['Margin'] = np.abs(PvA['Predicted'] - PvA['Actual'])"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPukn4BtEMdW"
      },
      "source": [
        "# Sum of all of the rows with errors.\n",
        "PvA_err = np.sum(PvA['Margin'])"
      ],
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOmWvh2oEOAF"
      },
      "source": [
        "# Length of the prediction set.\n",
        "PvA_len = len(PvA)"
      ],
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ZBN5s5EWP2"
      },
      "source": [
        "# Accuracy metric.\n",
        "PvA_acc = 1-(PvA_err/PvA_len)"
      ],
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQYkTfgOEa-V",
        "outputId": "bfb051e7-8935-42d2-e51e-309af100e875"
      },
      "source": [
        "# AORTD = Accuracy on Random Test Data\n",
        "print(\"AORTD: %.2f\" % (PvA_acc * 100) + \"%\")"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AORTD: 84.00%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}