{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_SPI_4121.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQcSHNruFfUzrza5ejFuAb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipsY4ETEii7U"
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# This is a swing at a custom neural network predicting soccer game results based on scores.\n",
        "# I've been in Keras for a few months, but am just starting to understand my way around.\n",
        "df = pd.read_csv(\"https://projects.fivethirtyeight.com/soccer-api/club/spi_matches.csv\").dropna()\n",
        "\n",
        "# We need to calculate one team's aggregate margin in order to determine the result.\n",
        "# We've chosen home for that calculation, but may as well get away as well for good measure.\n",
        "# Then, the absolute margin is the number of goals between the teams.\n",
        "df['h_marg'] = df['score1'] - df['score2']\n",
        "df['a_marg'] = df['score2'] - df['score1']\n",
        "df['margin'] = np.abs(df['score1'] - df['score2'])\n",
        "\n",
        "results = []\n",
        "\n",
        "for i in df['h_marg']:\n",
        "  if i > 0: # If the home team's margin is greater than 0, it's a home win.\n",
        "    results.append(\"HOME WIN\")\n",
        "  elif i < 0: # If the home team's margin is less than 0, it's an away win.\n",
        "    results.append(\"AWAY WIN\")\n",
        "  else: # Otherwise, it's a draw.\n",
        "    results.append(\"DRAW\")\n",
        "\n",
        "df['result'] = results"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1SHdtXTr0Y6"
      },
      "source": [
        "# The model gets a little confused the more numerical rows are fed into it, so we're keeping it simple.\n",
        "X = df[['score1', 'score2', 'margin']].reset_index(drop=True)\n",
        "y = df[['result']].copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6)"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDtLz66a8GTP"
      },
      "source": [
        "# Make some random test data (separate from the existing test dfs). First, we need to generate a function to turn the list into a dataframe.\n",
        "\n",
        "def to_df(l1):\n",
        "  df = pd.DataFrame(l1).reset_index(drop=True)\n",
        "  return df\n",
        "\n",
        "def to_cat(cats):\n",
        "  cat = to_categorical(pd.factorize(cats)[0])\n",
        "  return cat\n",
        "\n",
        "t1 = to_df([3,0,2,4,2,1,3,2,4,1,0,1,3,4,3,2,1,1,0,5,4,2,1,0,1,6,2,1,2,1,3,1,2,0,1,2,2,4,0,2,4,1,6,2,5,1,6,2,1,2,5,2,4,0,2,4,1,2])\n",
        "t2 = to_df([2,1,4,2,4,2,3,1,2,2,0,0,3,2,1,2,3,2,1,0,3,5,1,4,2,0,2,4,2,4,1,4,2,0,3,2,1,3,2,1,0,3,5,1,6,2,0,2,4,2,4,1,3,1,2,0,1,1])\n",
        "t3 = np.abs(t1) - np.abs(t2)\n",
        "t4 = t2 - t1\n",
        "t5 = t1 - t2\n",
        "\n",
        "t_res = []\n",
        "\n",
        "cols = ['score1', 'score2', 'margin', 'h_margin', 'a_margin']\n",
        "\n",
        "rand_X_test = pd.concat([t1, t2, t3, t4, t5], axis=1)\n",
        "rand_X_test.columns = cols\n",
        "\n",
        "for i in rand_X_test['h_margin']:\n",
        "  if i > 0: # If the home team's margin is greater than 0, it's a home win.\n",
        "    t_res.append(\"HOME WIN\")\n",
        "  elif i < 0: # If the home team's margin is less than 0, it's an away win.\n",
        "    t_res.append(\"AWAY WIN\")\n",
        "  else: # Otherwise, it's a draw.\n",
        "    t_res.append(\"DRAW\")\n",
        "\n",
        "rand_X_test['result'] = t_res\n",
        "rand_y_test = to_cat(rand_X_test['result'])\n",
        "rand_X_test = rand_X_test[['score1', 'score2', 'margin']]\n",
        "rand_reshape = rand_X_test.shape[0] * rand_X_test.shape[1]\n",
        "rand_X_test = np.asarray(rand_X_test)\n",
        "rand_y_test = np.asarray(rand_y_test)"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf8bCj8SD5R4"
      },
      "source": [
        "# Have to factorize the labels before converting them to categorical (e.g. 1.,0.,0.).\n",
        "y_train = to_cat(y_train['result'])\n",
        "y_test = to_cat(y_test['result'])\n",
        "\n",
        "# The train and test data will be easier to handle in array form.\n",
        "X_train = np.asarray(X_train)\n",
        "X_test = np.asarray(X_test)"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq9DJLDYD90V"
      },
      "source": [
        "# Two layers with a 3 unit output layer given the number of outcomes (HOME WIN, AWAY WIN, DRAW).\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF8WnMkZD-Km",
        "outputId": "e0199a2c-3c17-4354-99b7-b4bff5d5c9df"
      },
      "source": [
        "# Definitely excessive on the epochs. ¯\\_(ツ)_/¯\n",
        "model.fit(X_train, y_train, epochs=120, batch_size=128, validation_split=0.2, verbose=0)"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9f8d08b790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8_qMHJaEHDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100e8209-f44b-4343-93d7-1797bfba78e0"
      },
      "source": [
        "# Now we will predict results based on the random data we generated.\n",
        "predictions = model.predict(rand_X_test).round(2)\n",
        "predictions = np.asarray(predictions)"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9f9ab99560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbykGDS5EJYG"
      },
      "source": [
        "# Reshaping y_test and predictions to be directly compared 1v1 iteratively.\n",
        "predictions = predictions.reshape(rand_reshape,)\n",
        "predictions = pd.DataFrame(predictions).reset_index(drop=True)\n",
        "rand_y_test = rand_y_test.reshape(rand_reshape,)\n",
        "rand_y_test = pd.DataFrame(rand_y_test).reset_index(drop=True)\n",
        "PvA = pd.concat([rand_y_test, predictions], axis=1)\n",
        "PvA.columns = [\"Predicted\", \"Actual\"]"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_LV-1FeELB2"
      },
      "source": [
        "# If the margin is 0, there is no error. If the margin is 1, there was an error.\n",
        "PvA['Margin'] = np.abs(PvA['Predicted'] - PvA['Actual'])"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPukn4BtEMdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca21ea9-fcb4-4f0e-980c-e6c5f3aa17e1"
      },
      "source": [
        "# Sum of all of the rows with errors.\n",
        "PvA_err = np.sum(PvA['Margin'])\n",
        "print(PvA_err)\n",
        "print(len(PvA))"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42.0\n",
            "174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOmWvh2oEOAF"
      },
      "source": [
        "# Length of the prediction set.\n",
        "PvA_len = len(PvA)"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ZBN5s5EWP2"
      },
      "source": [
        "# Accuracy metric.\n",
        "PvA_acc = 1-(PvA_err/PvA_len)"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQYkTfgOEa-V",
        "outputId": "8041bc52-8cc2-42a3-a281-f54f9653818c"
      },
      "source": [
        "# AORTD = Accuracy on Random Test Data\n",
        "print(\"AORTD: %.2f\" % (PvA_acc * 100) + \"%\")"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AORTD: 75.86%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}